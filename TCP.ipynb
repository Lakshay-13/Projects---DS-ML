{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing Various required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import os\n",
    "import sys\n",
    "from sklearn import model_selection\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords list taken from intenet.\n",
    "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone',\n",
    "             'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount',\n",
    "             'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around',\n",
    "             'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before',\n",
    "             'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both',\n",
    "             'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'con', 'could', 'couldnt', 'cry', 'de',\n",
    "             'describe', 'detail', 'did', 'do', 'does', 'doing', 'don', 'done', 'down', 'due', 'during', 'each', 'eg',\n",
    "             'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone',\n",
    "             'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for',\n",
    "             'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had',\n",
    "             'has', 'hasnt', 'have', 'having', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed',\n",
    "             'interest', 'into', 'is', 'it', 'its', 'itself', 'just', 'keep', 'last', 'latter', 'latterly', 'least', 'less',\n",
    "             'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly',\n",
    "             'move', 'much', 'must', 'my', 'myself', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine',\n",
    "             'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once',\n",
    "             'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "             'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed', 'seeming',\n",
    "             'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', \n",
    "             'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system',\n",
    "             't', 'take', 'ten', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there',\n",
    "             'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thickv', 'thin', 'third', 'this',\n",
    "             'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward',\n",
    "             'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we',\n",
    "             'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby',\n",
    "             'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom',\n",
    "             'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself',\n",
    "             'yourselves']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriveData():\n",
    "    #Extracting Information from documents in 20_newsgroups\n",
    "    X = [] \n",
    "    Y = [] \n",
    "    for category in os.listdir('C:/Users/Lakshay/Desktop/20_newsgroups'):\n",
    "        for document in os.listdir('C:/Users/Lakshay/Desktop/20_newsgroups/'+category): \n",
    "            with open('C:/Users/Lakshay/Desktop/20_newsgroups/'+category+'/'+document, \"r\") as f:\n",
    "                X.append((document,f.read()))\n",
    "                Y.append(category)\n",
    "    return X,Y\n",
    "X,Y=retriveData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=model_selection.train_test_split(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDict(X_train):\n",
    "    #To create dictionary of words from the given documents with their corresponding frequency\n",
    "    vocab = {}\n",
    "    for i in range(len(X_train)):\n",
    "        word=X_train[i][1].lower()\n",
    "        #splitting the text into words\n",
    "        stripped=re.split(r'\\W+',word)\n",
    "        for word in stripped:\n",
    "            if (word.isalpha()) and (len(word)>2)  and (word not in stopwords): \n",
    "                if word in vocab:\n",
    "                    vocab[word]+=1\n",
    "                else:\n",
    "                    vocab[word]=1\n",
    "    return vocab\n",
    "vocab=createDict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XuUHOV95vHvr6u756K5ShpdGMmLACEQBoSMsbjYGRubWxzDJngjH5+11lFWJwnZ2MmeTSDZPZzEdtZ2Etvxro2RDWvZxzYQ31AwGGtlxsRrA8IGhAS6IowGCV2QZqTR3Pry7h/19kxLjDQtMTNVNf18zunTVW+9Vf32q5IevXVrc84hIiJSiVTUDRARkeRQaIiISMUUGiIiUjGFhoiIVEyhISIiFVNoiIhIxSoKDTN72cyeN7NnzexpXzbdzNaZ2Xb/3urLzcy+aGY7zGyjmS0t284KX3+7ma2YmK8kIiIT5XRGGu92zi1xzl3u528H1jvnFgLr/TzAjcBC/1oF3AVhyAB3Au8ArgDuLAWNiIgkw5s5PHUzsMZPrwFuKSv/hgs9AbSY2VzgemCdc+6Qc+4wsA644U18voiITLJ0hfUc8BMzc8DdzrnVwGzn3F4A59xeM5vl67YDu8vW7fJlJys/jpmtIhyhUFtb+7b09HkEKWN2vZ3G15p6isUiqZROQYH6opz6YoT6YsS2bdsOOufaJmLblYbG1c65PT4Y1pnZllPUHe1fd3eK8uMLwkBaDbBo0SK36L/cw/RpWb7+0SsqbOrU1NnZSUdHR9TNiAX1xQj1xQj1xQgz+81EbbuiWHbO7fHv+4EfEJ6T2OcPO+Hf9/vqXcD8stXnAXtOUX7qBppRKOr5WCIicTBmaJjZNDNrLE0D1wGbgLVA6QqoFcCDfnot8BF/FdUyoMcfxnoUuM7MWv0J8Ot82SkFKaOohyqKiMRCJYenZgM/MLNS/W87535sZhuAB8xsJfAK8EFf/2HgJmAH0Ad8FMA5d8jMPgFs8PX+zjl3aKwPD8woFk/jG4mIyIQZMzSccy8Bl45S/jpw7SjlDrjtJNu6F7j3dBpoBgWNNEREYiH2lxoEKaOocxoiIrGQiNDQSENEJB5iHxop00hDRCQuYh8a4dVTUbdCREQgAaGRMnSfhohITCQgNHSfhohIXMQ+NIKU7ggXEYmL2IdGSldPiYjERuxDIzBDmSEiEg+xDw2dCBcRiY/4h4bOaYiIxEbsQyPQ1VMiIrER/9DQSENEJDZiHxop3REuIhIb8Q8NQ4enRERiIvahEejnXkVEYiP2oZHS72mIiMRG7EMjMN0RLiISF/EPjZQuuRURiYvYh4aZUSxG3QoREYEEhEaQQoenRERiIv6hoaunRERiI/ahkUoZgK6gEhGJgdiHRmBhaOgQlYhI9GIfGsMjDYWGiEjk4h8aVjo8FXFDREQk/qER+Bbq8JSISPRiHxqlkYauoBIRiV7sQyPQ1VMiIrGRmNDQ4SkRkejFPjRGToQrNEREohb70NBIQ0QkPuIfGjoRLiISG7EPDZ8ZaKAhIhK92IfG8OEpjTRERCJXcWiYWWBmz5jZQ35+gZk9aWbbzex+M8v68ho/v8MvP7tsG3f48q1mdn0ln1sKjbxCQ0Qkcqcz0vgY8GLZ/GeAzzvnFgKHgZW+fCVw2Dl3HvB5Xw8zWwwsBy4CbgC+bGbBWB+a9beED+X1HBERkahVFBpmNg/4beBrft6A9wDf9VXWALf46Zv9PH75tb7+zcB9zrlB59wuYAdwxVifnU370CgoNEREopausN4XgL8EGv38DKDbOZf3811Au59uB3YDOOfyZtbj67cDT5Rts3ydYWa2ClgF0NbWxoubNwHw5IZf0b1zzIHJlNXb20tnZ2fUzYgF9cUI9cUI9cXkGDM0zOz9wH7n3K/MrKNUPEpVN8ayU60zUuDcamA1wKJFi9zbly6Bp5/grZdcytXnzRyruVNWZ2cnHR0dUTcjFtQXI9QXI9QXk6OSkcbVwAfM7CagFmgiHHm0mFnajzbmAXt8/S5gPtBlZmmgGThUVl5Svs5JDR+e0jkNEZHIjXlOwzl3h3NunnPubMIT2T91zn0YeAy41VdbATzop9f6efzynzrnnC9f7q+uWgAsBJ4a6/NLoTGo0BARiVyl5zRG81fAfWb2SeAZ4B5ffg/wTTPbQTjCWA7gnNtsZg8ALwB54DbnXGGsD6nRiXARkdg4rdBwznUCnX76JUa5+sk5NwB88CTrfwr41Ol8ZjYIT37r8JSISPRif0e4zmmIiMRH7EOjLhOONPqG8mPUFBGRiRb70CiNNHIFPUZERCRqsQ+N0lNui3rMrYhI5GIfGvqNcBGR+Ih/aJR+7lWZISISudiHRunwlH7uVUQkegkIDSNlOjwlIhIHsQ8NgJSZToSLiMRAMkIjZTo8JSISA4kIjcAMZYaISPQSERopg4LOaYiIRC4ZoZEyhYaISAwkIzTMcDo+JSISuUSERqAT4SIisZCI0AgvuY26FSIikpDQ0M19IiJxkIjQCFK6uU9EJA4SERopM/QT4SIi0UtGaKT0exoiInGQiNAI9OwpEZFYSERoZNMpBnM6PiUiErVEhEZDTZqjg7momyEiUvUSERqNtRmODuSjboaISNVLSGikFRoiIjGQkNDIcHRAh6dERKKWiNBoqk1zRCMNEZHIJSI0GmvTDOWLDOYLUTdFRKSqJSI0ajMBAAO67FZEJFKJCI10ygA9tFBEJGqJCI0gCJuZV2iIiEQqEaFRGmnoJ19FRKKViNAIfGjkizqnISISpUSEhkYaIiLxMGZomFmtmT1lZs+Z2WYz+1tfvsDMnjSz7WZ2v5llfXmNn9/hl59dtq07fPlWM7u+0kaOjDQUGiIiUapkpDEIvMc5dymwBLjBzJYBnwE+75xbCBwGVvr6K4HDzrnzgM/7epjZYmA5cBFwA/BlMwsqaWQ6FTZTIw0RkWiNGRou1OtnM/7lgPcA3/Xla4Bb/PTNfh6//FozM19+n3Nu0Dm3C9gBXFFJI4dHGgWFhohIlNKVVPIjgl8B5wFfAnYC3c650rM9uoB2P90O7AZwzuXNrAeY4cufKNts+Trln7UKWAXQ1tZGZ2cnL+4PP+apDRvY31zR4GTK6e3tpbOzM+pmxIL6YoT6YoT6YnJUFBrOuQKwxMxagB8AF45Wzb/bSZadrPzEz1oNrAZYtGiR6+jowG3dD7/ewCWXLWXpW1orafKU09nZSUdHR9TNiAX1xQj1xQj1xeQ4raunnHPdQCewDGgxs1LozAP2+OkuYD6AX94MHCovH2WdU6obfoyInj0lIhKlSq6eavMjDMysDngv8CLwGHCrr7YCeNBPr/Xz+OU/dc45X77cX121AFgIPFVJI7PpsJk5ndMQEYlUJYen5gJr/HmNFPCAc+4hM3sBuM/MPgk8A9zj698DfNPMdhCOMJYDOOc2m9kDwAtAHrjNH/YaU8ZfPZXL6+Y+EZEojRkazrmNwGWjlL/EKFc/OecGgA+eZFufAj51uo3MpMPTIbmCQkNEJEqJuCM84x9YOKTQEBGJVCJCIxvonIaISBwkIjQyw6GhkYaISJQSERqlq6cGdcmtiEikEhEapfs0+vVzryIikUpEaATDj0ZXaIiIRCkRoZEJSpfc6kS4iEiUEhEaZkaQMv1yn4hIxBIRGhD+ep9+hElEJFrJCg0dnhIRiVRyQiNIkdd9GiIikUpMaGSCFEMaaYiIRCoxoVGXTenmPhGRiCUnNDIB/QoNEZFIKTRERKRiiQmN2kxA/5BCQ0QkSokJjbpsoN8IFxGJWGJCozYdMKAHFoqIRCoxoVGX1TkNEZGoJSY0anUiXEQkcgkKjZTOaYiIRCwxoVGX0YlwEZGoJSY0ajMBuYLT86dERCKUmNAo/eTrQF6hISISlcSERm3W/064bvATEYlMckIjHTZV5zVERKKTmNCo8yMNhYaISHQSExq1aX94SqEhIhKZxIRGU10GgJ7+XMQtERGpXokJjYaaNADHBvMRt0REpHolLjR6B3V4SkQkKskJjVqNNEREopaY0JhWE54IP9w3FHFLRESqV2JCoyYdMLMhy97ugaibIiJStRITGgDtLXXs6emPuhkiIlVrzNAws/lm9piZvWhmm83sY758upmtM7Pt/r3Vl5uZfdHMdpjZRjNbWratFb7+djNbcbqNrcsGDOrX+0REIlPJSCMP/Ffn3IXAMuA2M1sM3A6sd84tBNb7eYAbgYX+tQq4C8KQAe4E3gFcAdxZCppK1aQDBvWUWxGRyIwZGs65vc65X/vpo8CLQDtwM7DGV1sD3OKnbwa+4UJPAC1mNhe4HljnnDvknDsMrANuOJ3GZtMpBnVHuIhIZNKnU9nMzgYuA54EZjvn9kIYLGY2y1drB3aXrdbly05WfuJnrCIcodDW1kZnZ+fwsp5DA3QfKR5XVi16e3ur8nuPRn0xQn0xQn0xOSoODTNrAL4HfNw5d8TMTlp1lDJ3ivLjC5xbDawGWLRokevo6Bhe9sjBjezaup/ysmrR2dlZld97NOqLEeqLEeqLyVHR1VNmliEMjG85577vi/f5w0749/2+vAuYX7b6PGDPKcorNr0hS3ffEM69IWtERGQSVHL1lAH3AC865z5XtmgtULoCagXwYFn5R/xVVMuAHn8Y61HgOjNr9SfAr/NlFZtenyVXcBzVXeEiIpGo5PDU1cB/BJ43s2d92V8DnwYeMLOVwCvAB/2yh4GbgB1AH/BRAOfcITP7BLDB1/s759yh02lsS334pNvuYzmaajOns6qIiIyDMUPDOfdzRj8fAXDtKPUdcNtJtnUvcO/pNLDc9GlZAA71DfGWGfVnuhkRETlDibojvKU+DA09f0pEJBoJCw1/eEqhISISiUSFRmtppHFMv94nIhKFRIVGc12GlMGhYxppiIhEIVGhEaSMWY21vHZEj0cXEYlCokIDYE5zLa/1KDRERKKQuNA4q6VWv6khIhKRxIXGzIYaXu/VOQ0RkSgkLjSmT8vS058jp9/VEBGZdIkLjTlNtQDs08lwEZFJl7jQaG+tA+DVwzqvISIy2ZIXGi0+NLoVGiIiky1xoXFWi0YaIiJRSVxo1GYCZjbUaKQhIhKBxIUGhOc1FBoiIpMvkaExr6VOh6dERCKQyNBob62jq7ufYlG/FS4iMpkSGRoLZzUwlC+y40Bv1E0REakqiQyNC+Y0AbBjv0JDRGQyJTI0Fs5uIBMYG7t6om6KiEhVSWRo1GYCFs9t4tndh6NuiohIVUlkaAAsmd/Cxq4eCjoZLiIyaRIbGkv/XSt9QwWefOn1qJsiIlI1Ehsa771wNimDJxQaIiKTJrGhMa0mzTltDbyw92jUTRERqRqJDQ2AxXOb2LynB+d0XkNEZDIkOjQue0sLe3sGeE0/yCQiMikSHRqXzGsB4Lnd3RG3RESkOiQ6NN7a3kR9NuBn2w5E3RQRkaqQ6NCoSQdcde5MfrRxLwO5QtTNERGZ8hIdGgC/c+lcjgzk+c3rfVE3RURkykt8aJw/uxGAzXv0HCoRkYk2JUKjsSbN4zqvISIy4RIfGkHKuPHiOTy6eZ+eQyUiMsHGDA0zu9fM9pvZprKy6Wa2zsy2+/dWX25m9kUz22FmG81sadk6K3z97Wa2Yjy/xBULZtCfK7Btn+4OFxGZSJWMNL4O3HBC2e3AeufcQmC9nwe4EVjoX6uAuyAMGeBO4B3AFcCdpaAZD+9aOJOUwYPP7hmvTYqIyCjGDA3n3OPAoROKbwbW+Ok1wC1l5d9woSeAFjObC1wPrHPOHXLOHQbW8cYgOmOzmmq5ZmEb//rcHh2iEhGZQOkzXG+2c24vgHNur5nN8uXtwO6yel2+7GTlb2BmqwhHKbS1tdHZ2VlRgy6qy/N49yCfe2A9b59zpl8rvnp7eyvui6lOfTFCfTFCfTE5xvtfVxulzJ2i/I2Fzq0GVgMsWrTIdXR0VPTB7yw61v7mMZ7tree/dSyrrLUJ0tnZSaV9MdWpL0aoL0aoLybHmV49tc8fdsK/7/flXcD8snrzgD2nKB83Qcq44a1z2LDrMIN53R0uIjIRzjQ01gKlK6BWAA+WlX/EX0W1DOjxh7EeBa4zs1Z/Avw6Xzaurjp3BkOFIht26bfDRUQmQiWX3H4H+CWwyMy6zGwl8GngfWa2HXifnwd4GHgJ2AF8FfgTAOfcIeATwAb/+jtfNq6uOncmNekU67fsG+9Ni4gIFZzTcM596CSLrh2lrgNuO8l27gXuPa3Wnaa6bMDV583kJ5v38dc3XUgmSPy9iyIisTLl/lX9/bfP59Xufn66Zf/YlUVE5LRMudB4zwWzmDEty7eefCXqpoiITDlTLjQyQYpV7zqHx7cd4P/tOBh1c0REppQpFxoAK646m/nT6/jvP9xE/5AuvxURGS9TMjRqMwF//+8vZtfBY3xu3daomyMiMmVMydAAeOfCNn53aTtf/bdd3PH9jYQXdomIyJsx9R7SVOazv3cJ6ZTxnad2c+0Fs3nv4tlRN0lEJNGm7EgDIB2k+MQtb6WxNs1ffW8jOw/0Rt0kEZFEm9KhAVCTDrhvVfgAw9+76xc8pvs3RETO2JQPDYCLzmrme398FbMba/mDNRv4xi9fjrpJIiKJVBWhAXD2zGn88LarufKcGXzyoRf52bYDUTdJRCRxqiY0IHw21f/60GWcPbOelV/foJv/REROU1WFBsCMhhr+5Y+u4ty2Blbc+xSf/fEWhvLFqJslIpIIVRcaAM11Gb6x8gp++5K5fLlzJx/8yi94+eCxqJslIhJ7VRkaALObavnn5Zfx5Q8v5YW9R7j1K7/g2d3dUTdLRCTWqjY0Sm66eC4P/9k7GcwX+f27f8maX7ysu8dFRE6i6kMDYOHsRh7+s3dy6fwW7ly7mT9c8zR7e/qjbpaISOwoNLz50+u5f9Uy/sf7F/PzHQe5/vOP8/1fdzGQ01NyRURKFBplzIyV1yzgwT+9mtlNtfzFA8/x9k/+Xz71oxc4cHQw6uaJiERuSj+w8ExdMKeJH3/8Xfx8x0EeeHo3X/23Xfzw2T38h8vn8d4LZ3NxezNp/f64iFQhhcZJBCnjt85v47fOb+OP3tXD/3zkRb7cuZMvPbaT5roMy98+n1vfNo/zZjVgZlE3V0RkUig0KnDxvGa+/Z+XcfjYEI9vP8C/PreXux9/ibsff4kL5jTyO5eexe8ubWduc13UTRURmVAKjdPQOi3LzUvauXlJO6929/PjTa+x9tlX+YdHt/IPj25lyfwWbrp4Drdc1s6sxtqomysiMu4UGmeovaWOldcsYOU1C9h5oJcfb3qNH23cy98/vIVPP7KFS+e3cO0Fs3jX+W0snNVIXTaIuskiIm+aQmMcnNvWwG3vPo/b3n0e2/Yd5Ucb97LuhX3807pt/ONPtpEJjIvbm3n3olksO3cGF7c3U5tRiIhI8ig0xtn5sxs5/32N/Pn7zmf/kQGe2HWI57u6eerlw/zTum2wDtIp46Kzmlh8VhOXzGvhknnNnD+7kYyuyBKRmFNoTKBZTbV84NKz+MClZwGw/8gAz+zuZmNXN0/tOsQjm17jO0/tBsKrtRbPbeK8WQ0smtPIRWc1ceHcJmY21ET5FUREjqPQmESzmmq5/qI5XH/RHACcc7x08Bgbu7rZsvcoL+w9wi93vs4Pnnl1eJ350+s4Z2YD9A2yLbWTBTMbaG+p4+yZ9dRn9ccnIpNL/+pEyMw4t62Bc9sa4LKR8u6+ITa9eoTNe3rY2NXDy68f45UDeX728Jbj1m+sSXNO2zQWzJzGnOY6ZjfVML+1nhkNWea11jOzIat7SERkXCk0YqilPss1C2dyzcKZw2WdnZ0sueIqXn69j67Dffzm9T5e7e5n14FjbHj5MPuO7CVfPP7pvOmUMae5lrOa65jZmGVmQw2zGmtoa6yhtT7LzMYaZk6robk+Q1NtWgEjImNSaCRIS32WJfVZlsxvecMy5xwHe4foOtw3/L7vyCB7e/p5rWeALXuPcqD3IEcH8qNuO50y6rIBzXUZWuuzzGqsoaU+S2t9hhkNNTTXZZjRkKWpNkPrtAyNtRma6zLUZQKClMJGpFooNKYIM6PNjyJOZTBfYP+RQQ73DfH6sSEOHBmku3+I7r4cfUMFuvuGONSX47UjA7yw9wjdfTn6x3jSb2t9hpb6LE21aeqyAdOnZanPpmmpy1CbCaivCWipyzKtJqClPkttOkVTXYaGmjS1mYDW+oye5SWSEAqNKlOTDpg/vZ750+srqu+c49hQgZ7+HIePDdHTnxt+HR3I0TtY4PCxIQ71DdE7kOfYYJ5t+3o5Npinuy/HQL5AJb9plQmMptoMddmA2kwwEjh+9FOfDWiuz1KTTvHKyzn2TXuFmnRANp2iLhswLZsmm05R4wOpJp0im04xLZvWSEhkHCk05JTMjIaaNA01adpbTv/ZWs45+nMFjvTnOTqQ48hAnoFcwY9s8vQNFTjSH45mDvflGMwV6M+FIdU3lGf/0QGODuTpHcwfd2jt/q3PV9h+yAQpMikjk05RlwmoywZkgxSZIEV9NgymTBCGTGNthkxgpFMpMmmjLhOGWDplpFNGEKTIBkZDTYYgZWHdIAyr+mzgy1IEvn69D7NMYOGyVIqUQkwSTKEhE8os/IezPptmTvObex6Xc46hQpH1jz3OJZe/g6F8kaFCkWODYfjkCkUGckWO9OcYKhQZzBU5OpBjqODIF4rkCkV6BwsM5Avk8uH8scECrx8bYihfZDBf5OhAnnyxSL7gyBXCsvGWTaeoCVIEgRFYGCZBykiZMa0mIJ1KkQ7C+dKydCoM79J86XVg3yDruzdRkw7DKFwHUnb8+qVAGy4zI5UaqVsakYXbCMvMvwe+zCwM0UxgmN9+ed3S9kvrNdSkh6dLdXWxRfJNemiY2Q3APwMB8DXn3Kcnuw2STGZGTTqgPmPMa63s8NqblfNhky868gVHvhiGUX8uDKmwzDGQK9A/VCBfdBSKbjh4jg3lyfnQKm2jbyjPUKFIwdctf/UO5ik6N7ydogvXGcgVOHRsaHhZsRi+9/UXeP7wHobyRYrOUSxCwYXrxfWn7lNlQYKF85lUippMULYs/PO2U8xn/AjP/HxPTz93bf3lccFlfr1U+FGkzKjJpMgG4XpG+XbBMFIpgBPKyj6/PhuE2wbw78OfFRYN1zWOL0/50DSz4f88MLzcytYvW7e8/ITtcdy8UZtJTfjh2EkNDTMLgC8B7wO6gA1mttY598JktkOkUhl/GCuuOjs76ejoGHWZc2Hw5ArhIcIwVBwFX+4cPuAcx3xYFV24XtEvK00XXbhe36DfjhtZvzRdLKubLxTpzxX9spHyke2PlDnH8EjPnbAt5xyO47eNfx/0o8XSNlIGjrI2la1H2fr9ucJx7XYnfE74+QAntKEYzvcN5fGbrEqTPdK4AtjhnHsJwMzuA24GFBoi48zMSAdGOqAqnrIcBuiVk/655UFXChNHWSCVphkJJVx4JWPOB3MpgI7fjisLpzduo3zbpXVLgXblZybu+052aLQDu8vmu4B3lFcws1XAKj87aGabJqltcTcTOBh1I2JCfTFCfTFCfTFi0URteLJDY7SDbccN8pxzq4HVAGb2tHPu8sloWNypL0aoL0aoL0aoL0aY2dMTte3JPljbBcwvm58H7JnkNoiIyBma7NDYACw0swVmlgWWA2snuQ0iInKGJvXwlHMub2Z/CjxKeMntvc65zadYZfXktCwR1Bcj1Bcj1Bcj1BcjJqwvzFXrdWMiInLa4nsBuoiIxI5CQ0REKhbb0DCzG8xsq5ntMLPbo27PeDOz+Wb2mJm9aGabzexjvny6ma0zs+3+vdWXm5l90ffHRjNbWratFb7+djNbEdV3erPMLDCzZ8zsIT+/wMye9N/rfn/xBGZW4+d3+OVnl23jDl++1cyuj+abvDlm1mJm3zWzLX7/uLJa9wsz+3P/92OTmX3HzGqrab8ws3vNbH/5/WrjuS+Y2dvM7Hm/zhfNKng4mHMudi/Ck+Q7gXOALPAcsDjqdo3zd5wLLPXTjcA2YDHwWeB2X3478Bk/fRPwCOG9LsuAJ335dOAl/97qp1uj/n5n2Cd/AXwbeMjPPwAs99NfAf7YT/8J8BU/vRy4308v9vtKDbDA70NB1N/rDPphDfCHfjoLtFTjfkF4M/AuoK5sf/hP1bRfAO8ClgKbysrGbV8AngKu9Os8Atw4Zpui7pSTdNSVwKNl83cAd0Tdrgn+zg8SPpNrKzDXl80Ftvrpu4EPldXf6pd/CLi7rPy4ekl5Ed6zsx54D/CQ34kPAukT9wnCq++u9NNpX89O3E/K6yXlBTT5fyjthPKq2y8YeYLEdP/n/BBwfbXtF8DZJ4TGuOwLftmWsvLj6p3sFdfDU6M9bqQ9orZMOD+Mvgx4EpjtnNsL4N9n+Won65Op0ldfAP4SKD2LfAbQ7Zwr/YhG+fca/s5+eY+vPxX64hzgAPB//KG6r5nZNKpwv3DOvQr8I/AKsJfwz/lXVOd+UW689oV2P31i+SnFNTTGfNzIVGFmDcD3gI87546cquooZe4U5YlhZu8H9jvnflVePEpVN8ayxPcF4f+QlwJ3OecuA44RHoI4mSnbF/5Y/c2Eh5TOAqYBN45StRr2i0qc7vc/o36Ja2hUxeNGzCxDGBjfcs593xfvM7O5fvlcYL8vP1mfTIW+uhr4gJm9DNxHeIjqC0CLmZVuQC3/XsPf2S9vBg4xNfqiC+hyzj3p579LGCLVuF+8F9jlnDvgnMsB3weuojr3i3LjtS90+ekTy08prqEx5R834q9SuAd40Tn3ubJFa4HS1Q0rCM91lMo/4q+QWAb0+KHpo8B1Ztbq/2d2nS9LDOfcHc65ec65swn/rH/qnPsw8Bhwq692Yl+U+uhWX9/58uX+KpoFwELCE32J4Zx7DdhtZqWnlF5L+NMBVbdfEB6WWmZm9f7vS6kvqm6/OMG47At+2VEzW+b79yNl2zq5qE/ynOLkz02EVxTtBP4m6vZMwPe7hnAouBF41r9uIjwGux7Y7t+n+/pG+ANWO4HngcvLtvUHwA7/+mjU3+1N9ksHI1egwxQsAAAAjklEQVRPnUP4l3sH8C9AjS+v9fM7/PJzytb/G99HW6ngSpA4voAlwNN+3/gh4RUvVblfAH8LbAE2Ad8kvAKqavYL4DuE53NyhCODleO5LwCX+77dCfxvTrgAY7SXHiMiIiIVi+vhKRERiSGFhoiIVEyhISIiFVNoiIhIxRQaIiJSMYWGiIhUTKEhIiIV+/87g2hw/BVxKwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plotGraph(vocab):\n",
    "    #To create a sorted vocab dictionary & plotting the graph for the same\n",
    "    sorted_vocab=[(i,vocab[i]) for i in sorted(vocab, key=vocab.get, reverse=True)]\n",
    "    features=[]\n",
    "    features_freq=[]\n",
    "    for i in range(len(sorted_vocab)):\n",
    "        features.append(i)\n",
    "        features_freq.append(sorted_vocab[i][1])\n",
    "    plt.plot(features,features_freq)\n",
    "    plt.axis([0,10000,0,5000])\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return sorted_vocab\n",
    "sorted_vocab=plotGraph(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking top 2000 words as our feature\n",
    "top_k_words=[sorted_vocab[i][0] for i in range(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_TrainTestDataset(X_train,X_test,top_k_words):\n",
    "    #To generate nD numpy array that contains of training & testing data with rows as no. of documents and column as top k words\n",
    "    X_train_dataset=np.zeros([len(X_train),len(top_k_words)],int)\n",
    "    X_test_dataset=np.zeros([len(X_test),len(top_k_words)],int)\n",
    "    for i in range(len(X_train)):\n",
    "        words=re.split(r'\\W+',X_train[i][1].lower())\n",
    "        for j in words:\n",
    "            if j in top_k_words:\n",
    "                X_train_dataset[i][top_k_words.index(j)]+=1\n",
    "\n",
    "    for i in range(len(X_test)):\n",
    "        words=re.split(r'\\W+',X_test[i][1].lower())\n",
    "        for j in words:\n",
    "            if j in top_k_words:\n",
    "                X_test_dataset[i][top_k_words.index(j)]+=1\n",
    "    return X_train_dataset,X_test_dataset\n",
    "\n",
    "X_train_dataset,X_test_dataset=generate_TrainTestDataset(X_train,X_test,top_k_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_dataset:-\n",
      "[[ 7  1  0 ...  0  0  0]\n",
      " [ 3  1  0 ...  0  0  0]\n",
      " [ 9  3  1 ...  0  0  0]\n",
      " ...\n",
      " [10  4  1 ...  0  0  0]\n",
      " [13  2  4 ...  0  0  0]\n",
      " [12  5  7 ...  0  0  0]]\n",
      "\n",
      "...................\n",
      "\n",
      "X_test_dataset:-\n",
      "[[11  1  1 ...  0  0  0]\n",
      " [ 4  1  0 ...  0  0  0]\n",
      " [ 6  1  0 ...  0  0  0]\n",
      " ...\n",
      " [ 9  3  1 ...  0  0  0]\n",
      " [ 2  2  7 ...  0  0  0]\n",
      " [ 1  1  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print('X_train_dataset:-')\n",
    "print(X_train_dataset)\n",
    "print()\n",
    "print('...................')\n",
    "print()\n",
    "print('X_test_dataset:-')\n",
    "print(X_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn's score on training data : 0.8971127558845102\n"
     ]
    }
   ],
   "source": [
    "#Using sklearn's Multinomial NB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_dataset,Y_train)\n",
    "Y_test_pred = clf.predict(X_test_dataset)\n",
    "sklearn_score_train = clf.score(X_train_dataset,Y_train)\n",
    "print(\"Sklearn's score on training data :\",sklearn_score_train)\n",
    "sklearn_score_test = clf.score(X_test_dataset,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn's score on testing data : 0.8522\n",
      "Confusion Matrix for testing data :-\n",
      "[[192   1   0   0   0   0   0   1   1   0   0   0   0   0   0   3   0   3\n",
      "    0  53]\n",
      " [  1 169  16  13  25   3   3   0   1   1   0   0   6   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3 198  16   5  17   6   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   2   8 200  35   2   3   1   0   0   0   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   4  18 232   0   6   0   0   0   0   0   2   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0  13  31   6   6 180   4   0   0   0   0   0   2   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   3   2   0 235   4   3   0   0   0   2   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   6 213   6   1   2   0   6   0   0   0   1   0\n",
      "    2   0]\n",
      " [  0   0   0   0   0   1   2   8 243   1   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   4   4 226  10   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   2   3   9 236   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   1   0   0   1   1   1   1   1   0 213   4   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   2   0   4   8   0   8   6   1   0   0   1 209   3   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   5   1   5   1   4   2   0   0  11 225   1   0   0   0\n",
      "    2   0]\n",
      " [  0   2   0   0   1   0   1   0   2   3   0   0   5   1 262   0   0   0\n",
      "    1   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   3   0   0   2   0   0   1   0 236   0\n",
      "   14  12]\n",
      " [  0   0   0   1   0   0   2   1   4   0   1   0   0   0   0   0   2 217\n",
      "   20   1]\n",
      " [  0   1   0   0   0   0   0   0   1   1   1   4   0   0   1   0  33  10\n",
      "  171  29]\n",
      " [ 40   0   0   0   0   0   0   0   0   0   0   0   1   1   1   9   9   1\n",
      "   13 155]]\n",
      "Classification report for testing data :-\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.82      0.76      0.79       254\n",
      "           comp.graphics       0.84      0.70      0.77       240\n",
      " comp.os.ms-windows.misc       0.76      0.80      0.78       247\n",
      "comp.sys.ibm.pc.hardware       0.77      0.79      0.78       254\n",
      "   comp.sys.mac.hardware       0.73      0.87      0.79       267\n",
      "          comp.windows.x       0.88      0.74      0.80       243\n",
      "            misc.forsale       0.83      0.93      0.88       252\n",
      "               rec.autos       0.88      0.90      0.89       237\n",
      "         rec.motorcycles       0.88      0.95      0.91       255\n",
      "      rec.sport.baseball       0.92      0.92      0.92       246\n",
      "        rec.sport.hockey       0.94      0.94      0.94       251\n",
      "               sci.crypt       0.96      0.94      0.95       226\n",
      "         sci.electronics       0.83      0.86      0.85       243\n",
      "                 sci.med       0.97      0.87      0.92       258\n",
      "               sci.space       0.96      0.94      0.95       279\n",
      "  soc.religion.christian       0.95      1.00      0.98       249\n",
      "      talk.politics.guns       0.84      0.88      0.86       268\n",
      "   talk.politics.mideast       0.94      0.87      0.90       249\n",
      "      talk.politics.misc       0.77      0.68      0.72       252\n",
      "      talk.religion.misc       0.62      0.67      0.64       230\n",
      "\n",
      "               micro avg       0.85      0.85      0.85      5000\n",
      "               macro avg       0.85      0.85      0.85      5000\n",
      "            weighted avg       0.86      0.85      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sklearn's score on testing data :\",sklearn_score_test)\n",
    "print(\"Confusion Matrix for testing data :-\")\n",
    "print(confusion_matrix(Y_test,Y_test_pred))\n",
    "print(\"Classification report for testing data :-\")\n",
    "print(classification_report(Y_test, Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Multinomial NB\n",
    "class MNB:\n",
    "    #creating class MNB\n",
    "    def __init__(self):\n",
    "        #To initialize the data members of class MNB\n",
    "        self.dict_category_count={}\n",
    "        self.classes = None\n",
    "    \n",
    "    def fit(self,x_train_dataset,y_train):\n",
    "        #To fit the training data into MNB \n",
    "        y_train=np.array(y_train)\n",
    "        self.dict_category_count[\"total_doc\"]=len(y_train)\n",
    "        self.classes=set(y_train)\n",
    "        classes=set(y_train)\n",
    "        for current_class in classes:\n",
    "            total_word=0\n",
    "            x_train_current=x_train_dataset[y_train==current_class]\n",
    "            rows_x_train_current=x_train_current.shape[0]\n",
    "            self.dict_category_count[current_class]={}\n",
    "            for feature in top_k_words:\n",
    "                feature_total_word=(x_train_current[:,top_k_words.index(feature)]).sum()\n",
    "                self.dict_category_count[current_class][feature]=feature_total_word\n",
    "                total_word+=feature_total_word\n",
    "            self.dict_category_count[current_class][\"word_in_class\"]=total_word\n",
    "            self.dict_category_count[current_class][\"length_class\"]=rows_x_train_current    \n",
    "\n",
    "    def probability(self,x,class_name):    \n",
    "        #finding probability of occurances of given class_name\n",
    "        prob=np.log(self.dict_category_count[class_name][\"length_class\"])-np.log(self.dict_category_count[\"total_doc\"])\n",
    "        feature=list(self.dict_category_count[class_name].keys())\n",
    "        x_total_words=len(feature)-2 \n",
    "        for j in range (x_total_words):\n",
    "            xj=x[j]\n",
    "            if xj==0:\n",
    "                current_prob=0\n",
    "            else:\n",
    "                #Extra addition part is Laplace Correction\n",
    "                num=self.dict_category_count[class_name][feature[j]]+1\n",
    "                den=self.dict_category_count[class_name][\"word_in_class\"]+x_total_words\n",
    "                current_prob=(np.log(num)-np.log(den))\n",
    "            prob+=current_prob\n",
    "        return prob\n",
    "\n",
    "    def predict_tup(self,x):\n",
    "        #To find the best class\n",
    "        first_run=True\n",
    "        best_prob=None\n",
    "        best_class=None\n",
    "        for current_class in self.classes:\n",
    "            if current_class==\"total_doc\":\n",
    "                continue\n",
    "            prob=self.probability(x,current_class)\n",
    "            #best_class will have maximum probability\n",
    "            if first_run or prob>best_prob:\n",
    "                best_prob=prob\n",
    "                first_run=False\n",
    "                best_class=current_class\n",
    "        return best_class\n",
    "\n",
    "    def predict(self,x_test):\n",
    "        #To predict the class\n",
    "        y_pred=[]\n",
    "        for x in x_test:\n",
    "            x_class=self.predict_tup(x)\n",
    "            y_pred.append(x_class)\n",
    "        return y_pred\n",
    "\n",
    "    def score(self,y_test,y_pred):\n",
    "        #To calculate the mean accuracy\n",
    "        count = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == y_test[i]:\n",
    "                count+=1\n",
    "        return count/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our score on testing data : 0.8726\n",
      "Confusion Matrix for testing data :-\n",
      "[[192   1   0   0   0   0   0   1   1   0   0   0   0   0   0   3   0   3\n",
      "    0  53]\n",
      " [  1 169  16  13  25   3   3   0   1   1   0   0   6   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3 198  16   5  17   6   0   0   0   0   0   2   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   2   8 200  35   2   3   1   0   0   0   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   4   4  18 232   0   6   0   0   0   0   0   2   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0  13  31   6   6 180   4   0   0   0   0   0   2   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   3   2   0 235   4   3   0   0   0   2   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   6 213   6   1   2   0   6   0   0   0   1   0\n",
      "    2   0]\n",
      " [  0   0   0   0   0   1   2   8 243   1   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   1   4   4 226  10   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   2   3   9 236   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   1   0   0   1   1   1   1   1   0 213   4   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   2   0   4   8   0   8   6   1   0   0   1 209   3   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   5   1   5   1   4   2   0   0  11 225   1   0   0   0\n",
      "    2   0]\n",
      " [  0   2   0   0   1   0   1   0   2   3   0   0   5   1 262   0   0   0\n",
      "    1   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   0   0   3   0   0   2   0   0   1   0 236   0\n",
      "   14  12]\n",
      " [  0   0   0   1   0   0   2   1   4   0   1   0   0   0   0   0   2 217\n",
      "   20   1]\n",
      " [  0   1   0   0   0   0   0   0   1   1   1   4   0   0   1   0  33  10\n",
      "  171  29]\n",
      " [ 40   0   0   0   0   0   0   0   0   0   0   0   1   1   1   9   9   1\n",
      "   13 155]]\n",
      "Classification report for testing data :-\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.78      0.85      0.81       254\n",
      "           comp.graphics       0.77      0.76      0.77       240\n",
      " comp.os.ms-windows.misc       0.83      0.77      0.80       247\n",
      "comp.sys.ibm.pc.hardware       0.75      0.86      0.80       254\n",
      "   comp.sys.mac.hardware       0.82      0.88      0.85       267\n",
      "          comp.windows.x       0.87      0.77      0.82       243\n",
      "            misc.forsale       0.88      0.93      0.91       252\n",
      "               rec.autos       0.93      0.94      0.93       237\n",
      "         rec.motorcycles       0.97      0.98      0.97       255\n",
      "      rec.sport.baseball       0.97      0.96      0.96       246\n",
      "        rec.sport.hockey       0.97      0.97      0.97       251\n",
      "               sci.crypt       0.95      0.94      0.94       226\n",
      "         sci.electronics       0.82      0.92      0.87       243\n",
      "                 sci.med       0.96      0.91      0.93       258\n",
      "               sci.space       0.98      0.91      0.94       279\n",
      "  soc.religion.christian       0.97      1.00      0.98       249\n",
      "      talk.politics.guns       0.82      0.93      0.87       268\n",
      "   talk.politics.mideast       0.95      0.92      0.93       249\n",
      "      talk.politics.misc       0.78      0.67      0.72       252\n",
      "      talk.religion.misc       0.66      0.56      0.61       230\n",
      "\n",
      "               micro avg       0.87      0.87      0.87      5000\n",
      "               macro avg       0.87      0.87      0.87      5000\n",
      "            weighted avg       0.87      0.87      0.87      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using Multinomial NB\n",
    "clf2 = MNB()\n",
    "clf2.fit(X_train_dataset,Y_train)\n",
    "our_Y_test_pred = clf2.predict(X_test_dataset)\n",
    "our_score_test = clf2.score(our_Y_test_pred,Y_test)  \n",
    "print(\"Our score on testing data :\",our_score_test)\n",
    "print(\"Confusion Matrix for testing data :-\")\n",
    "print(confusion_matrix(Y_test,Y_test_pred))\n",
    "print(\"Classification report for testing data :-\")\n",
    "print(classification_report(Y_test, our_Y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of our model on test data: 0.8726\n",
      "Score of inbuilt sklearn's MultinomialNB on the same data : 0.8522\n"
     ]
    }
   ],
   "source": [
    "#Compare Results of your implementation of Multinomial Naive Bayes with one in Sklearn\n",
    "print(\"Score of our model on test data:\",our_score_test)\n",
    "print(\"Score of inbuilt sklearn's MultinomialNB on the same data :\",sklearn_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self implemented model performed slightly better than inbuilt sklearn Multinomial Naive Bayes Model in text classification of 20_newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
